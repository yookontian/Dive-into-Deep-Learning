{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The EOS vector often represents the final input vector x n to \"cue\" the encoder \n",
    " \n",
    " that the input sequence has ended and also defines the end of the target sequence.\n",
    " \n",
    " As soon as the EOS is sampled from a logit vector, the generation is complete. \n",
    " \n",
    " The BOS vector represents the input vector y 0  fed to the decoder RNN at the very first decoding step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/My/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)olve/main/source.spm: 100%|██████████| 768k/768k [00:00<00:00, 795kB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 797k/797k [00:00<00:00, 798kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.27M/1.27M [00:00<00:00, 1.28MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 17.1kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.33k/1.33k [00:00<00:00, 516kB/s]\n",
      "/home/tian/mambaforge/envs/My/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Downloading pytorch_model.bin: 100%|██████████| 298M/298M [00:03<00:00, 82.8MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 121kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Ich will ein Auto kaufen</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/My/lib/python3.10/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "\n",
    "# create ids of encoded input vectors\n",
    "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# translate example\n",
    "output_ids = model.generate(input_ids)[0]\n",
    "\n",
    "# decode and print\n",
    "print(tokenizer.decode(output_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(58101, 512, padding_idx=58100)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=58101, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/My/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input embeddings torch.Size([1, 7, 512]). Length of encoder_hidden_states torch.Size([1, 7, 512])\n",
      "Is encoding for `I` equal to its perturbed version?:  False\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# create ids of encoded input vectors\n",
    "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# pass input_ids to encoder\n",
    "encoder_hidden_states = model.base_model.encoder(input_ids, return_dict=True).last_hidden_state\n",
    "\n",
    "# change the input slightly and pass to encoder\n",
    "input_ids_perturbed = tokenizer(\"I want to buy a house\", return_tensors=\"pt\").input_ids\n",
    "encoder_hidden_states_perturbed = model.base_model.encoder(input_ids_perturbed, return_dict=True).last_hidden_state\n",
    "\n",
    "# compare shape and encoding of first vector\n",
    "print(f\"Length of input embeddings {embeddings(input_ids).shape}. Length of encoder_hidden_states {encoder_hidden_states.shape}\")\n",
    "\n",
    "# compare values of word embedding of \"I\" for input_ids and perturbed input_ids\n",
    "print(\"Is encoding for `I` equal to its perturbed version?: \", torch.allclose(encoder_hidden_states[0, 0], encoder_hidden_states_perturbed[0, 0], atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/My/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder input vectors torch.Size([1, 5, 512]). Shape of decoder logits torch.Size([1, 5, 58101])\n",
      "Is encoding for `Ich` equal to its perturbed version?:  True\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# create token ids for encoder input\n",
    "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# pass input token ids to encoder\n",
    "encoder_output_vectors = model.base_model.encoder(input_ids, return_dict=True).last_hidden_state\n",
    "\n",
    "# create token ids for decoder input\n",
    "decoder_input_ids = tokenizer(\"<pad> Ich will ein\", return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "\n",
    "# pass decoder input ids and encoded input vectors to decoder\n",
    "decoder_output_vectors = model.base_model.decoder(decoder_input_ids, encoder_hidden_states=encoder_output_vectors).last_hidden_state\n",
    "\n",
    "# derive embeddings by multiplying decoder outputs with embedding weights\n",
    "lm_logits = torch.nn.functional.linear(decoder_output_vectors, embeddings.weight, bias=model.final_logits_bias)\n",
    "\n",
    "# change the decoder input slightly\n",
    "decoder_input_ids_perturbed = tokenizer(\"<pad> Ich will das\", return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "decoder_output_vectors_perturbed = model.base_model.decoder(decoder_input_ids_perturbed, encoder_hidden_states=encoder_output_vectors).last_hidden_state\n",
    "lm_logits_perturbed = torch.nn.functional.linear(decoder_output_vectors_perturbed, embeddings.weight, bias=model.final_logits_bias)\n",
    "\n",
    "# compare shape and encoding of first vector\n",
    "print(f\"Shape of decoder input vectors {embeddings(decoder_input_ids).shape}. Shape of decoder logits {lm_logits.shape}\")\n",
    "\n",
    "# compare values of word embedding of \"I\" for input_ids and perturbed input_ids\n",
    "print(\"Is encoding for `Ich` equal to its perturbed version?: \", torch.allclose(lm_logits[0, 0], lm_logits_perturbed[0, 0], atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "\n",
    "# create ids of encoded input vectors\n",
    "input_ids = tokenizer(\"She is so pretty\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# create BOS token\n",
    "decoder_input_ids = tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "assert decoder_input_ids[0, 0].item() == model.config.decoder_start_token_id, \"`decoder_input_ids` should correspond to `model.config.decoder_start_token_id`\"\n",
    "\n",
    "# STEP 1\n",
    "\n",
    "# pass input_ids to encoder and to decoder and pass BOS token to decoder to retrieve first logit\n",
    "outputs = model(input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)\n",
    "\n",
    "# get encoded sequence\n",
    "encoded_sequence = (outputs.encoder_last_hidden_state,)\n",
    "# get logits\n",
    "lm_logits = outputs.logits\n",
    "\n",
    "# sample last token with highest prob\n",
    "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
    "\n",
    "# concat\n",
    "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
    "\n",
    "# STEP 2\n",
    "\n",
    "# reuse encoded_inputs and pass BOS + \"Ich\" to decoder to second logit\n",
    "lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
    "\n",
    "# sample last token with highest prob again\n",
    "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
    "\n",
    "# concat again\n",
    "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
    "\n",
    "# STEP 3\n",
    "lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
    "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
    "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
    "\n",
    "# let's see what we have generated so far!\n",
    "print(f\"Generated so far: {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}\")\n",
    "\n",
    "# This can be written in a loop as well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
